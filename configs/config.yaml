model:
  name: "distilbert/distilgpt2"
  learning_rate: 5e-5
  warmup_steps: 500
  path: "gs://mlops_team13_bucket/models"

data:
  dataset_name: "wikitext"
  config_name: "wikitext-2-raw-v1"
  split: "validation"
  batch_size: 4
  max_length: 512
  subset_size: 100
  limit: 1

training:
  max_epochs: 1
  accelerator: "gpu"  # Change to "gpu" if available
  devices: 1
  precision: 16-mixed
  accumulate_grad_batches: 1
  limit_train_batches: 1
  limit_val_batches: 1
  output_path: "outputs"

logging:
  log_every_n_steps: 1

